{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comformer D2R2 Single Property Prediction Notebook\n",
    "\n",
    "This notebook generates predictions for the single property model, performs de-standardization, and calculates metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "current_dir = os.path.dirname(os.path.abspath('.'))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "0\n",
      "2.5.1\n",
      "12.1\n",
      "True\n",
      "NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should be >0\n",
    "print(torch.cuda.current_device())  # Should print device ID\n",
    "print(torch.__version__)  # Check PyTorch version\n",
    "print(torch.version.cuda) # Should be 11.7, but CUDA system is 12.1\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.get_device_name(0))  # Should print GPU name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data and Model\n",
    "\n",
    "First, we need to load the ground truth data and test-train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 29481\n",
      "Number of validation samples: 3685\n",
      "Number of test samples: 3685\n"
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "data_path = os.path.join(current_dir, 'data', 'DFT_data.csv')\n",
    "split_path = os.path.join(current_dir, 'output', 'D2R2_WF_bottom', 'ids_train_val_test.json')\n",
    "model_path = os.path.join(current_dir, 'output', 'D2R2_WF_bottom', 'checkpoint_395.pt')\n",
    "\n",
    "# Load data\n",
    "data_df = pd.read_csv(data_path)\n",
    "\n",
    "# Load the train/val/test split information\n",
    "with open(split_path, 'r') as f:\n",
    "    train_test_val = json.loads(f.read())\n",
    "\n",
    "# Create ID column in the data dataframe to match with prediction IDs\n",
    "data_df['id'] = data_df[\"mpid\"].astype(str) + data_df[\"miller\"].astype(str) + data_df[\"term\"].astype(str)\n",
    "\n",
    "# Filter data by sets\n",
    "train_data_df = data_df[data_df['id'].isin(train_test_val['id_train'])]\n",
    "val_data_df = data_df[data_df['id'].isin(train_test_val['id_val'])]\n",
    "test_data_df = data_df[data_df['id'].isin(train_test_val['id_test'])]\n",
    "\n",
    "# Get counts\n",
    "print(f\"Number of training samples: {len(train_data_df)}\")\n",
    "print(f\"Number of validation samples: {len(val_data_df)}\")\n",
    "print(f\"Number of test samples: {len(test_data_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Initialize the Model\n",
    "\n",
    "Now we'll load the trained model to generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary modules\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomformer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m iComformer, iComformerConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcomformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_torch_test_loader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Load the model configuration\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from ..comformer.models.comformer import iComformer, iComformerConfig\n",
    "from ..comformer.data import get_torch_test_loader\n",
    "\n",
    "# Load the model configuration\n",
    "config = iComformerConfig(\n",
    "    name=\"iComformer\",\n",
    "    output_features=1,  # Single property model\n",
    "    use_angle=True\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = iComformer(config).to(device)\n",
    "\n",
    "# Load trained weights\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {model_path}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Predictions for Test Data\n",
    "\n",
    "Create a data loader for the test data and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the property we're predicting (same as in training script)\n",
    "target_prop = \"WF_bottom\"\n",
    "\n",
    "# Create test loader\n",
    "test_loader = get_torch_test_loader(\n",
    "    dataset=\"D2R2_surface_data\",\n",
    "    target=target_prop,\n",
    "    batch_size=64,\n",
    "    atom_features=\"cgcnn\",\n",
    "    cutoff=4.0,\n",
    "    max_neighbors=25,\n",
    "    id_tag=\"id\",\n",
    "    pyg_input=True,\n",
    "    use_lattice=True,\n",
    "    data_path=data_path,\n",
    "    ids=train_test_val['id_test']\n",
    ")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "targets = []\n",
    "ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        g, lg, _, target = batch\n",
    "        output = model([g.to(device), lg.to(device), _.to(device)])\n",
    "        \n",
    "        # Get batch ID\n",
    "        batch_ids = test_loader.dataset.ids\n",
    "        ids.extend(batch_ids)\n",
    "        \n",
    "        # Collect predictions and targets\n",
    "        output = output.cpu().numpy().tolist()\n",
    "        if not isinstance(output, list):\n",
    "            output = [output]\n",
    "            \n",
    "        target = target.cpu().numpy().flatten().tolist()\n",
    "        if len(target) == 1:\n",
    "            target = [target[0]]\n",
    "            \n",
    "        predictions.extend(output)\n",
    "        targets.extend(target)\n",
    "\n",
    "# Create predictions dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    f'{target_prop}': targets,\n",
    "    f'{target_prop}_pred': predictions\n",
    "})\n",
    "\n",
    "print(f\"Generated {len(results_df)} predictions\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Calculate Standardization Parameters\n",
    "\n",
    "Calculate and verify the standardization parameters used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std for the target property using the training data\n",
    "mean_train = train_data_df[target_prop].mean()\n",
    "std_train = train_data_df[target_prop].std()\n",
    "\n",
    "print(f\"Training Data Statistics for {target_prop}:\")\n",
    "print(f\"Mean = {mean_train:.6f}, Std = {std_train:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify Standardization Parameters using Linear Regression\n",
    "\n",
    "Following the approach in result_analysis.ipynb, we'll use linear regression to verify the standardization parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a merged dataframe with both standardized and original values\n",
    "merged_df = pd.merge(results_df, test_data_df[['id', target_prop]], on='id', suffixes=('_std', '_orig'))\n",
    "\n",
    "# Calculate what the standardized values should be using our mean and std\n",
    "merged_df[f'{target_prop}_calculated_std'] = (merged_df[f'{target_prop}_orig'] - mean_train) / std_train\n",
    "\n",
    "# Calculate the difference between our calculated standardized values and those in the prediction file\n",
    "diff = merged_df[f'{target_prop}_calculated_std'] - merged_df[f'{target_prop}_std']\n",
    "\n",
    "print(f\"Verification for {target_prop}:\")\n",
    "print(f\"  Mean difference: {diff.mean():.6f}\")\n",
    "print(f\"  Max absolute difference: {diff.abs().max():.6f}\")\n",
    "print(f\"  Standard deviation of difference: {diff.std():.6f}\")\n",
    "\n",
    "# Determine if we need to recalculate standardization parameters\n",
    "max_acceptable_diff = 0.05  # Threshold for acceptable difference\n",
    "recalculate = diff.abs().max() > max_acceptable_diff\n",
    "\n",
    "if recalculate:\n",
    "    print(\"\\nLarge discrepancy detected. Recalculating using linear regression...\")\n",
    "    \n",
    "    # Use linear regression to find the actual parameters\n",
    "    X = merged_df[f'{target_prop}_std'].values.reshape(-1, 1)\n",
    "    y = merged_df[f'{target_prop}_orig'].values\n",
    "    \n",
    "    reg = LinearRegression().fit(X, y)\n",
    "    a = reg.coef_[0]  # This is std_train\n",
    "    b = reg.intercept_  # This is mean_train\n",
    "    \n",
    "    print(f\"{target_prop}: y = {a:.6f} * x + {b:.6f}, R² = {reg.score(X, y):.6f}\")\n",
    "    \n",
    "    print(f\"\\nOriginal mean_train: {mean_train:.6f}\")\n",
    "    print(f\"Original std_train: {std_train:.6f}\")\n",
    "    print(f\"Regression mean_train: {b:.6f}\")\n",
    "    print(f\"Regression std_train: {a:.6f}\")\n",
    "    \n",
    "    # Update mean and std if regression values are significantly different\n",
    "    mean_diff = abs(mean_train - b)\n",
    "    std_diff = abs(std_train - a)\n",
    "    \n",
    "    if mean_diff > 0.1 or std_diff > 0.1:\n",
    "        print(\"\\nUsing regression values for standardization parameters\")\n",
    "        mean_train = b\n",
    "        std_train = a\n",
    "    else:\n",
    "        print(\"\\nDifferences are small. Keeping original calculated values.\")\n",
    "else:\n",
    "    print(\"\\nStandardization parameters appear correct. No need to recalculate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. De-standardize the Predictions\n",
    "\n",
    "Now we'll convert the standardized predictions back to the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-standardize the target and prediction values\n",
    "results_df[f\"{target_prop}_destd\"] = results_df[target_prop] * std_train + mean_train\n",
    "results_df[f\"{target_prop}_pred_destd\"] = results_df[f\"{target_prop}_pred\"] * std_train + mean_train\n",
    "\n",
    "# Display the first few rows with de-standardized values\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate Performance Metrics\n",
    "\n",
    "Calculate MAE and MAPE for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE between de-standardized predictions and original values\n",
    "mae = mean_absolute_error(\n",
    "    results_df[f\"{target_prop}_destd\"], \n",
    "    results_df[f\"{target_prop}_pred_destd\"]\n",
    ")\n",
    "\n",
    "# Calculate MAPE (avoiding division by zero)\n",
    "non_zero_mask = results_df[f\"{target_prop}_destd\"] != 0\n",
    "if non_zero_mask.sum() > 0:\n",
    "    mape = np.mean(\n",
    "        np.abs(\n",
    "            (results_df[f\"{target_prop}_destd\"][non_zero_mask] - \n",
    "             results_df[f\"{target_prop}_pred_destd\"][non_zero_mask]) / \n",
    "            np.abs(results_df[f\"{target_prop}_destd\"][non_zero_mask])\n",
    "        )\n",
    "    ) * 100\n",
    "else:\n",
    "    mape = np.nan\n",
    "\n",
    "print(f\"Performance Metrics for {target_prop}:\")\n",
    "print(f\"MAE: {mae:.6f}\")\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Results\n",
    "\n",
    "Create visualizations to help understand the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(\n",
    "    results_df[f\"{target_prop}_destd\"], \n",
    "    results_df[f\"{target_prop}_pred_destd\"], \n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "# Add perfect prediction line\n",
    "min_val = min(results_df[f\"{target_prop}_destd\"].min(), results_df[f\"{target_prop}_pred_destd\"].min())\n",
    "max_val = max(results_df[f\"{target_prop}_destd\"].max(), results_df[f\"{target_prop}_pred_destd\"].max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "plt.title(f'{target_prop} Predictions\\nMAE: {mae:.4f}, MAPE: {mape:.2f}%')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "output_dir = os.path.join(current_dir, 'output', 'D2R2_WF_bottom')\n",
    "plt.savefig(os.path.join(output_dir, f'{target_prop}_prediction_vs_ground_truth.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error histogram\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate errors\n",
    "errors = results_df[f\"{target_prop}_destd\"] - results_df[f\"{target_prop}_pred_destd\"]\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(errors, bins=30, alpha=0.7)\n",
    "plt.xlabel('Error (Ground Truth - Prediction)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'{target_prop} Error Distribution\\nMean: {errors.mean():.4f}, Std: {errors.std():.4f}')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save figure\n",
    "plt.savefig(os.path.join(output_dir, f'{target_prop}_error_distribution.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results\n",
    "\n",
    "Save the predictions and metrics to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a CSV file\n",
    "output_file = os.path.join(output_dir, f'{target_prop}_predictions.csv')\n",
    "results_df.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Save metrics to a JSON file\n",
    "metrics = {\n",
    "    'property': target_prop,\n",
    "    'mae': float(mae),\n",
    "    'mape': float(mape),\n",
    "    'mean_error': float(errors.mean()),\n",
    "    'std_error': float(errors.std()),\n",
    "    'standardization': {\n",
    "        'mean': float(mean_train),\n",
    "        'std': float(std_train)\n",
    "    }\n",
    "}\n",
    "\n",
    "metrics_file = os.path.join(output_dir, f'{target_prop}_metrics.json')\n",
    "with open(metrics_file, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"Metrics saved to {metrics_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Fix Missing Predictions Issue\n",
    "\n",
    "Create a properly formatted predictions.json file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a proper predictions.json file similar to the multi-property format\n",
    "predictions_list = []\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    predictions_list.append({\n",
    "        'id': row['id'],\n",
    "        'target': row[target_prop],\n",
    "        'predictions': row[f'{target_prop}_pred']\n",
    "    })\n",
    "\n",
    "predictions_file = os.path.join(output_dir, 'predictions.json')\n",
    "with open(predictions_file, 'w') as f:\n",
    "    json.dump(predictions_list, f, indent=2)\n",
    "print(f\"Predictions JSON saved to {predictions_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
