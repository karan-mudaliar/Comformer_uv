{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comformer Tutorial: Graph Neural Networks for Material Properties\n",
    "\n",
    "This tutorial provides a comprehensive introduction to Comformer, a Graph Neural Network (GNN) model for predicting material properties. We'll walk through the entire process from loading data to inference, explaining each step in detail.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Comformer is a GNN model designed for materials science applications. Unlike traditional Convolutional Neural Networks (CNNs) that operate on regular grid-like data (e.g., images), GNNs can process irregular data structures like atomic structures in materials. This makes them particularly suitable for predicting material properties based on atomic arrangements.\n",
    "\n",
    "In this tutorial, we'll explore:\n",
    "1. Loading material data\n",
    "2. Creating graph representations of materials\n",
    "3. Understanding feature generation\n",
    "4. Setting up and training the Comformer model\n",
    "5. Making predictions with the trained model\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Environment\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from jarvis.core.atoms import Atoms, pmg_to_atoms\n",
    "from pymatgen.core import Structure\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up paths to the Comformer code\n",
    "import sys\n",
    "sys.path.append('../') # Add parent directory to path\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and Exploring the Dataset\n",
    "\n",
    "We'll use the sample data provided in the repository to demonstrate how to work with material data. Let's first load and explore this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset\n",
    "sample_data_path = \"../sample_data/surface_prop_data_set_top_bottom.csv\"\n",
    "df = pd.read_csv(sample_data_path, on_bad_lines=\"skip\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring dataset properties\n",
    "\n",
    "Let's explore the dataset to better understand what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the available columns\n",
    "print(\"Available columns:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "# Basic statistics of numerical properties\n",
    "print(\"\\nBasic statistics of target properties:\")\n",
    "targets = ['WF_bottom', 'WF_top', 'cleavage_energy']\n",
    "print(df[targets].describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(df[targets].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the target distributions\n",
    "\n",
    "Let's visualize the distribution of our target properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of target properties\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for i, target in enumerate(targets):\n",
    "    sns.histplot(df[target], kde=True, ax=axes[i])\n",
    "    axes[i].set_title(f'Distribution of {target}')\n",
    "    axes[i].set_xlabel(target)\n",
    "    axes[i].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the structure data\n",
    "\n",
    "The key element of materials datasets is the atomic structure data. Let's examine how the structure data is stored and represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the structure data format\n",
    "print(\"Example of structure data:\")\n",
    "print(df['slab'].iloc[0][:500] + '...')\n",
    "\n",
    "# Parse one structure to understand the format\n",
    "structure_dict = eval(df['slab'].iloc[0])\n",
    "structure = Structure.from_dict(structure_dict)\n",
    "atoms = pmg_to_atoms(structure)\n",
    "\n",
    "print(f\"\\nStructure information:\")\n",
    "print(f\"Number of atoms: {len(atoms)}\")\n",
    "print(f\"Chemical species: {set(atoms.elements)}\")\n",
    "print(f\"Lattice parameters: a={atoms.lattice.a:.4f}, b={atoms.lattice.b:.4f}, c={atoms.lattice.c:.4f}\")\n",
    "print(f\"Angles: alpha={atoms.lattice.alpha:.2f}, beta={atoms.lattice.beta:.2f}, gamma={atoms.lattice.gamma:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation for Graph Neural Networks\n",
    "\n",
    "Now let's implement the key functions from the Comformer codebase to prepare the data for the GNN. This involves:\n",
    "1. Creating a unique ID for each structure\n",
    "2. Preprocessing the structure data\n",
    "3. Converting structures to graphs\n",
    "\n",
    "Let's implement these steps one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a unique ID for each structure\n",
    "df[\"jid\"] = df[\"mpid\"].astype(str) + df[\"miller\"].astype(str) + df[\"term\"].astype(str)\n",
    "\n",
    "# Step 2: Rename 'slab' to 'atoms' if needed (to match the expected format)\n",
    "if \"slab\" in df.columns:\n",
    "    df = df.rename(columns={\"slab\": \"atoms\"})\n",
    "\n",
    "# Choose our target property\n",
    "target = \"WF_top\"  # We'll predict the work function of the top surface\n",
    "\n",
    "# Print information about the prepared dataset\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "print(f\"Target property: {target}\")\n",
    "print(f\"Target range: {df[target].min():.4f} to {df[target].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Graph Generation Explained\n",
    "\n",
    "Before we convert the structures to graphs, let's understand what this process involves:\n",
    "\n",
    "1. **Nodes**: Each atom in the structure becomes a node in the graph\n",
    "2. **Node Features**: Each node has features like atomic number, which captures chemical information\n",
    "3. **Edges**: Connections between atoms, usually based on distance (e.g., k-nearest neighbors)\n",
    "4. **Edge Features**: Properties of the connections, like interatomic distances\n",
    "\n",
    "Now let's implement the graph conversion function from the Comformer codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules from Comformer\n",
    "from comformer.graphs import PygGraph\n",
    "\n",
    "def atoms_to_graph(atoms_string, \n",
    "                   neighbor_strategy=\"k-nearest\",\n",
    "                   cutoff=8.0,\n",
    "                   max_neighbors=12,\n",
    "                   use_canonize=False,\n",
    "                   use_lattice=False,\n",
    "                   use_angle=False):\n",
    "    \"\"\"Convert a structure string to a PyG graph.\"\"\"\n",
    "    # Parse the structure\n",
    "    structure = pmg_to_atoms(Structure.from_dict(eval(atoms_string)))\n",
    "    \n",
    "    # Create the graph\n",
    "    graph = PygGraph.atom_dgl_multigraph(\n",
    "        structure,\n",
    "        neighbor_strategy=neighbor_strategy,\n",
    "        cutoff=cutoff,\n",
    "        atom_features=\"atomic_number\",\n",
    "        max_neighbors=max_neighbors,\n",
    "        compute_line_graph=False,\n",
    "        use_canonize=use_canonize,\n",
    "        use_lattice=use_lattice,\n",
    "        use_angle=use_angle,\n",
    "    )\n",
    "    return graph\n",
    "\n",
    "# Convert a sample structure to a graph to examine\n",
    "sample_graph = atoms_to_graph(df['atoms'].iloc[0])\n",
    "\n",
    "# Examine the graph properties\n",
    "print(f\"Graph properties:\")\n",
    "print(f\"Number of nodes: {sample_graph.x.shape[0]}\")\n",
    "print(f\"Node feature dimension: {sample_graph.x.shape[1]}\")\n",
    "print(f\"Number of edges: {sample_graph.edge_index.shape[1]}\")\n",
    "print(f\"Edge feature dimension: {sample_graph.edge_attr.shape[1] if hasattr(sample_graph, 'edge_attr') else 'N/A'}\")\n",
    "\n",
    "# Show the first few node features\n",
    "print(\"\\nFirst 5 node features:\")\n",
    "print(sample_graph.x[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create a Custom Dataset Class\n",
    "\n",
    "Now, let's implement a simplified version of the `PygStructureDataset` class to prepare our dataset for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data.batch import Batch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimplePygStructureDataset(Dataset):\n",
    "    \"\"\"A simplified version of the Comformer PygStructureDataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, df, target, atom_features=\"atomic_number\"):\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.ids = self.df[\"jid\"]\n",
    "        \n",
    "        # Convert targets to tensor\n",
    "        self.labels = torch.tensor(self.df[target].values).float()\n",
    "        \n",
    "        # Normalize labels\n",
    "        self.mean = self.labels.mean()\n",
    "        self.std = self.labels.std()\n",
    "        self.labels = (self.labels - self.mean) / self.std\n",
    "        \n",
    "        print(f\"Target normalization: mean={self.mean:.4f}, std={self.std:.4f}\")\n",
    "        \n",
    "        # Convert structures to graphs\n",
    "        print(\"Converting structures to graphs...\")\n",
    "        self.graphs = []\n",
    "        for i, atoms_str in enumerate(self.df['atoms']):\n",
    "            if i % 5 == 0:  # Print progress every 5 samples\n",
    "                print(f\"Processing structure {i+1}/{len(self.df)}\")\n",
    "            graph = atoms_to_graph(atoms_str)\n",
    "            self.graphs.append(graph)\n",
    "        \n",
    "        # Get atomic features lookup\n",
    "        from jarvis.core.specie import chem_data, get_node_attributes\n",
    "        \n",
    "        # Load atomic features\n",
    "        max_z = max(v[\"Z\"] for v in chem_data.values())\n",
    "        template = get_node_attributes(\"C\", atom_features)\n",
    "        features = np.zeros((1 + max_z, len(template)))\n",
    "        \n",
    "        for element, v in chem_data.items():\n",
    "            z = v[\"Z\"]\n",
    "            x = get_node_attributes(element, atom_features)\n",
    "            if x is not None:\n",
    "                features[z, :] = x\n",
    "        \n",
    "        # Update node features in graphs\n",
    "        for g in self.graphs:\n",
    "            z = g.x\n",
    "            g.atomic_number = z\n",
    "            z = z.type(torch.IntTensor).squeeze()\n",
    "            f = torch.tensor(features[z]).type(torch.FloatTensor)\n",
    "            if g.x.size(0) == 1:\n",
    "                f = f.unsqueeze(0)\n",
    "            g.x = f\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        graph = self.graphs[idx]\n",
    "        label = self.labels[idx]\n",
    "        return graph, graph, graph, label  # Return graph triplet and label as expected by model\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate(samples):\n",
    "        \"\"\"Collate function for DataLoader.\"\"\"\n",
    "        graphs, line_graphs, lattice, labels = map(list, zip(*samples))\n",
    "        batched_graph = Batch.from_data_list(graphs)\n",
    "        batched_line_graph = Batch.from_data_list(line_graphs)\n",
    "        return batched_graph, batched_line_graph, batched_line_graph, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create Train/Validation/Test Splits\n",
    "\n",
    "Let's create data splits for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(df, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1, seed=42):\n",
    "    \"\"\"Split the dataframe into train, validation, and test sets.\"\"\"\n",
    "    # Ensure ratios sum to 1\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-10\n",
    "    \n",
    "    # Create random indices\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(len(df))\n",
    "    \n",
    "    # Calculate split sizes\n",
    "    n_train = int(len(df) * train_ratio)\n",
    "    n_val = int(len(df) * val_ratio)\n",
    "    \n",
    "    # Create splits\n",
    "    train_indices = indices[:n_train]\n",
    "    val_indices = indices[n_train:n_train+n_val]\n",
    "    test_indices = indices[n_train+n_val:]\n",
    "    \n",
    "    # Create dataframes\n",
    "    train_df = df.iloc[train_indices].reset_index(drop=True)\n",
    "    val_df = df.iloc[val_indices].reset_index(drop=True)\n",
    "    test_df = df.iloc[test_indices].reset_index(drop=True)\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# Create data splits\n",
    "train_df, val_df, test_df = get_data_splits(df)\n",
    "\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Validation set size: {len(val_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Create Data Loaders\n",
    "\n",
    "For a small tutorial, let's use a smaller subset of the data to speed up processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a smaller subset for this tutorial\n",
    "small_train_df = train_df.head(10)  # Just use 10 samples for training\n",
    "small_val_df = val_df.head(5)      # 5 samples for validation\n",
    "small_test_df = test_df.head(5)    # 5 samples for testing\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimplePygStructureDataset(small_train_df, target)\n",
    "val_dataset = SimplePygStructureDataset(small_val_df, target)\n",
    "test_dataset = SimplePygStructureDataset(small_test_df, target)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_dataset.collate,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=val_dataset.collate,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_dataset.collate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding the Model Architecture\n",
    "\n",
    "Now, let's implement a simplified version of the Comformer model to understand its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter\n",
    "\n",
    "class RBFExpansion(nn.Module):\n",
    "    \"\"\"Radial basis function expansion module.\"\"\"\n",
    "    \n",
    "    def __init__(self, vmin, vmax, bins):\n",
    "        super().__init__()\n",
    "        self.vmin = vmin\n",
    "        self.vmax = vmax\n",
    "        self.bins = bins\n",
    "        self.gap = (vmax - vmin) / bins\n",
    "        self.centers = nn.Parameter(\n",
    "            torch.linspace(vmin, vmax, bins), requires_grad=False\n",
    "        )\n",
    "        \n",
    "    def forward(self, dist):\n",
    "        # Compute RBF values\n",
    "        dist = dist.view(-1, 1)\n",
    "        centers = self.centers.view(1, -1)\n",
    "        diff = dist - centers\n",
    "        coef = -0.5 / (self.gap**2)\n",
    "        rbf = torch.exp(coef * (diff**2))\n",
    "        return rbf\n",
    "\n",
    "class SimpleComformerAttention(nn.Module):\n",
    "    \"\"\"A simplified version of the Comformer attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, edge_channels, heads=1):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        \n",
    "        # Key, query, value projections\n",
    "        self.lin_key = nn.Linear(in_channels, heads * out_channels)\n",
    "        self.lin_query = nn.Linear(in_channels, heads * out_channels)\n",
    "        self.lin_value = nn.Linear(in_channels, heads * out_channels)\n",
    "        self.lin_edge = nn.Linear(edge_channels, heads * out_channels)\n",
    "        \n",
    "        # Output projection\n",
    "        self.lin_concat = nn.Linear(heads * out_channels, out_channels)\n",
    "        \n",
    "        # Message passing networks\n",
    "        self.lin_msg_update = nn.Sequential(\n",
    "            nn.Linear(out_channels * 3, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Key update network\n",
    "        self.key_update = nn.Sequential(\n",
    "            nn.Linear(out_channels * 3, out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(out_channels, out_channels)\n",
    "        )\n",
    "        \n",
    "        # Other layers\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.bn_att = nn.BatchNorm1d(out_channels)\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Edge indices [2, num_edges]\n",
    "            edge_attr: Edge features [num_edges, edge_channels]\n",
    "            \n",
    "        Returns:\n",
    "            Updated node features [num_nodes, out_channels]\n",
    "        \"\"\"\n",
    "        H, C = self.heads, self.out_channels\n",
    "        \n",
    "        # Project features\n",
    "        query = self.lin_query(x).view(-1, H, C)\n",
    "        key = self.lin_key(x).view(-1, H, C)\n",
    "        value = self.lin_value(x).view(-1, H, C)\n",
    "        edge = self.lin_edge(edge_attr).view(-1, H, C)\n",
    "        \n",
    "        # Get source and destination node indices\n",
    "        src_idx, dst_idx = edge_index\n",
    "        \n",
    "        # Get features for source and destination nodes\n",
    "        query_i = query[dst_idx]\n",
    "        key_i = key[dst_idx]\n",
    "        key_j = key[src_idx]\n",
    "        value_i = value[dst_idx]\n",
    "        value_j = value[src_idx]\n",
    "        \n",
    "        # Compute attention scores\n",
    "        key_j = self.key_update(torch.cat((key_i, key_j, edge), dim=-1))\n",
    "        alpha = (query_i * key_j) / (C ** 0.5)\n",
    "        \n",
    "        # Compute message updates\n",
    "        out = self.lin_msg_update(torch.cat((value_i, value_j, edge), dim=-1))\n",
    "        out = out * self.sigmoid(self.bn_att(alpha.view(-1, C)).view(-1, H, C))\n",
    "        \n",
    "        # Aggregate messages\n",
    "        out = out.view(-1, H * C)\n",
    "        out = self.lin_concat(out)\n",
    "        \n",
    "        # Sum messages for each destination node\n",
    "        out = scatter(out, dst_idx, dim=0, reduce=\"sum\", dim_size=x.size(0))\n",
    "        \n",
    "        # Apply residual connection and batch norm\n",
    "        return self.softplus(x + self.bn(out))\n",
    "\n",
    "class SimpleComformer(nn.Module):\n",
    "    \"\"\"A simplified version of the Comformer model.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 atom_input_features=92,\n",
    "                 edge_features=256,\n",
    "                 node_features=256,\n",
    "                 output_features=1,\n",
    "                 n_conv_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Node embedding\n",
    "        self.atom_embedding = nn.Linear(atom_input_features, node_features)\n",
    "        \n",
    "        # Edge embedding\n",
    "        self.rbf = nn.Sequential(\n",
    "            RBFExpansion(vmin=-4.0, vmax=0.0, bins=edge_features),\n",
    "            nn.Linear(edge_features, node_features),\n",
    "            nn.Softplus(),\n",
    "        )\n",
    "        \n",
    "        # Attention layers\n",
    "        self.att_layers = nn.ModuleList([\n",
    "            SimpleComformerAttention(\n",
    "                in_channels=node_features,\n",
    "                out_channels=node_features,\n",
    "                edge_channels=node_features)\n",
    "            for _ in range(n_conv_layers)\n",
    "        ])\n",
    "        \n",
    "        # Readout layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(node_features, node_features),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_out = nn.Linear(node_features, output_features)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            data: Tuple of (graph, line_graph, lattice)\n",
    "            \n",
    "        Returns:\n",
    "            Predicted properties\n",
    "        \"\"\"\n",
    "        graph, _, _ = data\n",
    "        \n",
    "        # Embed nodes\n",
    "        node_features = self.atom_embedding(graph.x)\n",
    "        \n",
    "        # Compute edge features\n",
    "        edge_feat = -0.75 / torch.norm(graph.edge_attr, dim=1)\n",
    "        edge_features = self.rbf(edge_feat)\n",
    "        \n",
    "        # Apply attention layers\n",
    "        for att_layer in self.att_layers:\n",
    "            node_features = att_layer(node_features, graph.edge_index, edge_features)\n",
    "        \n",
    "        # Global pooling (average node features per graph)\n",
    "        features = scatter(node_features, graph.batch, dim=0, reduce=\"mean\")\n",
    "        \n",
    "        # Apply final layers\n",
    "        features = self.fc(features)\n",
    "        out = self.fc_out(features)\n",
    "        \n",
    "        return torch.squeeze(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "Now let's set up a simple training loop for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "# Determine the atom input features from the first graph\n",
    "atom_features_dim = train_dataset.graphs[0].x.shape[1]\n",
    "\n",
    "model = SimpleComformer(\n",
    "    atom_input_features=atom_features_dim,\n",
    "    edge_features=64,  # Reduced for faster training\n",
    "    node_features=64,  # Reduced for faster training\n",
    "    output_features=1,\n",
    "    n_conv_layers=2    # Reduced for faster training\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n",
    "print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        # Move batch to device\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        g, lg, lattice, target = batch\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model([g, lg, lattice])\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move batch to device\n",
    "            batch = [b.to(device) for b in batch]\n",
    "            g, lg, lattice, target = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model([g, lg, lattice])\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Average losses\n",
    "    train_loss /= len(train_loader)\n",
    "    val_loss /= len(val_loader)\n",
    "    \n",
    "    # Save losses\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, num_epochs+1), train_losses, 'b-', label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), val_losses, 'r-', label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation and Inference\n",
    "\n",
    "Now let's evaluate our model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_targets = []\n",
    "test_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move batch to device\n",
    "        batch = [b.to(device) for b in batch]\n",
    "        g, lg, lattice, target = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model([g, lg, lattice])\n",
    "        \n",
    "        # Denormalize predictions and targets\n",
    "        output_denorm = output.cpu().numpy() * test_dataset.std.numpy() + test_dataset.mean.numpy()\n",
    "        target_denorm = target.cpu().numpy() * test_dataset.std.numpy() + test_dataset.mean.numpy()\n",
    "        \n",
    "        test_targets.append(target_denorm.item())\n",
    "        test_predictions.append(output_denorm.item())\n",
    "\n",
    "# Calculate MAE\n",
    "mae = np.mean(np.abs(np.array(test_targets) - np.array(test_predictions)))\n",
    "print(f\"Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Plot predictions vs targets\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(test_targets, test_predictions)\n",
    "plt.plot([min(test_targets), max(test_targets)], [min(test_targets), max(test_targets)], 'r--')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.title(f'Test Set: Predictions vs True Values\\nMAE: {mae:.4f}')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Making Predictions on New Structures\n",
    "\n",
    "Now let's see how to use our trained model to make predictions on new structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_property(model, structure_string, mean, std):\n",
    "    \"\"\"Predict property for a new structure.\"\"\"\n",
    "    # Convert structure to graph\n",
    "    graph = atoms_to_graph(structure_string)\n",
    "    \n",
    "    # Create a batch with just this graph\n",
    "    batch = [graph, graph, graph]\n",
    "    batch = [b.to(device) for b in batch]\n",
    "    \n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(batch)\n",
    "        # Denormalize prediction\n",
    "        prediction_denorm = prediction.cpu().numpy() * std + mean\n",
    "    \n",
    "    return prediction_denorm.item()\n",
    "\n",
    "# Let's make a prediction on a structure from the test set\n",
    "sample_structure = test_df['atoms'].iloc[0]\n",
    "true_value = test_df[target].iloc[0]\n",
    "\n",
    "# Predict\n",
    "prediction = predict_property(model, sample_structure, test_dataset.mean.item(), test_dataset.std.item())\n",
    "\n",
    "print(f\"True {target}: {true_value:.4f}\")\n",
    "print(f\"Predicted {target}: {prediction:.4f}\")\n",
    "print(f\"Absolute error: {abs(true_value - prediction):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Key Insights\n",
    "\n",
    "In this tutorial, we've walked through the process of using Comformer, a Graph Neural Network for predicting material properties. Here's a summary of the key steps and insights:\n",
    "\n",
    "### Data Processing Pipeline\n",
    "1. **Loading Crystal Structures**: We loaded structures from CSV files, where each structure is represented as a dictionary containing atomic positions and lattice information.\n",
    "2. **Converting to Graphs**: We converted crystal structures to graphs where atoms are nodes and bonds are edges.\n",
    "3. **Feature Generation**: Node features represent atomic properties, while edge features capture bond information.\n",
    "\n",
    "### Model Architecture\n",
    "The Comformer model consists of:\n",
    "1. **Node and Edge Embedding Layers**: Convert raw node and edge features to embeddings.\n",
    "2. **Attention Layers**: Update node features based on their neighbors and the edge features.\n",
    "3. **Readout Layer**: Pool node features to get a graph-level representation.\n",
    "4. **Output Layer**: Predict the target property from the graph representation.\n",
    "\n",
    "### Key Advantages of GNNs for Materials\n",
    "1. **Permutation Invariance**: The model is invariant to the order of atoms in the structure.\n",
    "2. **Local Structure Awareness**: The model can capture local bonding patterns and environments.\n",
    "3. **Scalability**: Can handle structures with different numbers of atoms.\n",
    "4. **Transferability**: Knowledge learned from one material can be transferred to others.\n",
    "\n",
    "### Practical Considerations\n",
    "1. **Data Normalization**: Important for stable training and accurate predictions.\n",
    "2. **Graph Construction**: The choice of cutoff and number of neighbors impacts model performance.\n",
    "3. **Model Size**: Deep GNNs can capture complex relationships but may overfit on small datasets.\n",
    "\n",
    "GNNs like Comformer represent a powerful approach for predicting material properties directly from atomic structures, enabling rapid screening and discovery of novel materials with desired properties."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}