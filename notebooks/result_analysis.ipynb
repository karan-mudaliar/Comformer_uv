{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2R2 Model Results Analysis\n",
    "\n",
    "This notebook correctly analyzes the predictions from the Comformer model by properly handling the standardization/de-standardization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from git import Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "First, we'll load both the predictions and ground truth data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parent directory of the repo\n",
    "repo = Repo(os.getcwd(), search_parent_directories=True)\n",
    "repo_root = repo.git.rev_parse(\"--show-toplevel\")\n",
    "\n",
    "# Construct the path to the data files\n",
    "data_path = os.path.join(repo_root, 'data', 'DFT_data.csv')\n",
    "split_path = os.path.join(repo_root, \"output/D2R2_multi3/ids_train_val_test.json\")\n",
    "results_path = os.path.join(repo_root,\"output/D2R2_multi3/multi_out_predictions.json\")\n",
    "model_path = os.path.join(repo_root, \"output/D2R2_multi3/checkpoint_350.pt\")\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Split path: {split_path}\")\n",
    "print(f\"Results path: {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_df = pd.read_csv(data_path)\n",
    "results_df = pd.read_json(results_path)\n",
    "\n",
    "# Load the train/val/test split information\n",
    "with open(split_path, 'r') as f:\n",
    "    train_test_val = json.loads(f.read())\n",
    "\n",
    "# Create ID column in the data dataframe to match with prediction IDs\n",
    "data_df['id'] = data_df[\"mpid\"].astype(str) + data_df[\"miller\"].astype(str) + data_df[\"term\"].astype(str)\n",
    "\n",
    "# Filter to get only test data\n",
    "test_data_df = data_df[data_df['id'].isin(train_test_val['id_test'])]\n",
    "\n",
    "print(f\"Number of test samples: {len(test_data_df)}\")\n",
    "print(f\"Number of predictions: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Training Set Standardization Parameters\n",
    "\n",
    "We need to obtain the correct standardization parameters (mean and std) that were used during training. We'll calculate these directly from the data using the train/val/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training data using the train IDs from the split file\n",
    "train_data_df = data_df[data_df['id'].isin(train_test_val['id_train'])]\n",
    "val_data_df = data_df[data_df['id'].isin(train_test_val['id_val'])]\n",
    "\n",
    "print(f\"Number of training samples: {len(train_data_df)}\")\n",
    "print(f\"Number of validation samples: {len(val_data_df)}\")\n",
    "print(f\"Number of test samples: {len(test_data_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the property columns we're analyzing\n",
    "target_cols = [\"WF_bottom\", \"WF_top\", \"cleavage_energy\"]\n",
    "\n",
    "# Calculate mean and std for each property using the training data\n",
    "mean_train = train_data_df[target_cols].mean().values\n",
    "std_train = train_data_df[target_cols].std().values\n",
    "\n",
    "print(\"Training Data Statistics:\")\n",
    "for i, col in enumerate(target_cols):\n",
    "    print(f\"{col}: Mean = {mean_train[i]:.6f}, Std = {std_train[i]:.6f}\")\n",
    "\n",
    "print(\"\\nMean Training Values:\", mean_train)\n",
    "print(\"Std Training Values:\", std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract and Parse the Prediction Data\n",
    "\n",
    "Now we'll parse the prediction data and convert the standardized values back to the original scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the target and prediction arrays from the results dataframe\n",
    "target_cols = [\"WF_bottom\", \"WF_top\", \"cleavage_energy\"]\n",
    "pred_cols = [\"WF_bottom_pred\", \"WF_top_pred\", \"cleavage_energy_pred\"]\n",
    "\n",
    "# Create new columns for the standardized values\n",
    "results_df[target_cols] = pd.DataFrame(results_df['target'].tolist(), index=results_df.index)\n",
    "results_df[pred_cols] = pd.DataFrame(results_df['predictions'].tolist(), index=results_df.index)\n",
    "\n",
    "# Remove the original list columns\n",
    "results_df = results_df.drop(columns=['target', 'predictions'])\n",
    "\n",
    "# Display the first few rows of the transformed dataframe\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. De-standardize the Prediction Data\n",
    "\n",
    "Now we'll convert the standardized values back to the original scale using the correct mean and std values from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-standardize the target and prediction values\n",
    "for i, col in enumerate(target_cols):\n",
    "    # De-standardize target\n",
    "    results_df[f\"{col}_destd\"] = results_df[col] * std_train[i] + mean_train[i]\n",
    "    \n",
    "    # De-standardize prediction\n",
    "    pred_col = pred_cols[i]\n",
    "    results_df[f\"{pred_col}_destd\"] = results_df[pred_col] * std_train[i] + mean_train[i]\n",
    "\n",
    "# Display the first few rows with de-standardized values\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge with Original Data to Compare Ground Truth\n",
    "\n",
    "Now we'll merge the de-standardized predictions with the original data to compare with the ground truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions with original data\n",
    "merged_results = pd.merge(results_df, test_data_df[['id'] + target_cols], on='id', suffixes=('', '_orig'))\n",
    "\n",
    "# Display the first few rows of the merged dataframe\n",
    "merged_results[['id'] + \n",
    "               [f\"{col}_destd\" for col in target_cols] + \n",
    "               [f\"{col}_pred_destd\" for col in target_cols] + \n",
    "               target_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify De-standardization\n",
    "\n",
    "Let's check if our de-standardized values match the original ground truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare de-standardized targets with original values\n",
    "for col in target_cols:\n",
    "    destd_col = f\"{col}_destd\"\n",
    "    \n",
    "    # Calculate the difference between de-standardized and original values\n",
    "    diff = merged_results[destd_col] - merged_results[col]\n",
    "    \n",
    "    print(f\"{col} de-standardization check:\")\n",
    "    print(f\"  Mean difference: {diff.mean():.6f}\")\n",
    "    print(f\"  Max difference: {diff.abs().max():.6f}\")\n",
    "    print(f\"  Standard deviation of difference: {diff.std():.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a merged dataframe with both standardized and original values\n",
    "merged_data = []\n",
    "for i, row in results_df.iterrows():\n",
    "    id_val = row['id']\n",
    "    original_row = test_data_df[test_data_df['id'] == id_val]\n",
    "    \n",
    "    if len(original_row) > 0:\n",
    "        merged_data.append({\n",
    "            'id': id_val,\n",
    "            'WF_bottom_std': row['WF_bottom'],\n",
    "            'WF_top_std': row['WF_top'],\n",
    "            'cleavage_energy_std': row['cleavage_energy'],\n",
    "            'WF_bottom_orig': original_row['WF_bottom'].values[0],\n",
    "            'WF_top_orig': original_row['WF_top'].values[0],\n",
    "            'cleavage_energy_orig': original_row['cleavage_energy'].values[0]\n",
    "        })\n",
    "\n",
    "merged_df = pd.DataFrame(merged_data)\n",
    "\n",
    "# Calculate what the standardized values should be using our mean and std\n",
    "for i, prop in enumerate(target_cols):\n",
    "    merged_df[f'{prop}_calculated_std'] = (merged_df[f'{prop}_orig'] - mean_train[i]) / std_train[i]\n",
    "    \n",
    "    # Calculate the difference between our calculated standardized values and those in the prediction file\n",
    "    diff = merged_df[f'{prop}_calculated_std'] - merged_df[f'{prop}_std']\n",
    "    \n",
    "    print(f\"Verification for {prop}:\")\n",
    "    print(f\"  Mean difference: {diff.mean():.6f}\")\n",
    "    print(f\"  Max absolute difference: {diff.abs().max():.6f}\")\n",
    "    print(f\"  Standard deviation of difference: {diff.std():.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the differences are significant, we may need to use regression to find the actual parameters used\n",
    "# during training. This code will only execute if we detect large discrepancies.\n",
    "\n",
    "max_acceptable_diff = 0.05  # Threshold for acceptable difference\n",
    "\n",
    "recalculate = False\n",
    "for i, prop in enumerate(target_cols):\n",
    "    diff = merged_df[f'{prop}_calculated_std'] - merged_df[f'{prop}_std']\n",
    "    if diff.abs().max() > max_acceptable_diff:\n",
    "        recalculate = True\n",
    "        print(f\"Large discrepancy detected for {prop}. Will recalculate using regression.\")\n",
    "\n",
    "if recalculate:\n",
    "    print(\"\\nRecalculating standardization parameters using linear regression...\")\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Initialize arrays to store calculated mean and std values\n",
    "    mean_train_reg = []\n",
    "    std_train_reg = []\n",
    "    \n",
    "    for prop in target_cols:\n",
    "        X = merged_df[f'{prop}_std'].values.reshape(-1, 1)\n",
    "        y = merged_df[f'{prop}_orig'].values\n",
    "        \n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        a = reg.coef_[0]  # This is std_train\n",
    "        b = reg.intercept_  # This is mean_train\n",
    "        \n",
    "        mean_train_reg.append(b)\n",
    "        std_train_reg.append(a)\n",
    "        \n",
    "        print(f\"{prop}: y = {a:.6f} * x + {b:.6f}, RÂ² = {reg.score(X, y):.6f}\")\n",
    "    \n",
    "    mean_train_reg = np.array(mean_train_reg)\n",
    "    std_train_reg = np.array(std_train_reg)\n",
    "    \n",
    "    print(f\"\\nOriginal mean_train: {mean_train}\")\n",
    "    print(f\"Original std_train: {std_train}\")\n",
    "    print(f\"Regression mean_train: {mean_train_reg}\")\n",
    "    print(f\"Regression std_train: {std_train_reg}\")\n",
    "    \n",
    "    # Use the regression values if they're significantly different\n",
    "    mean_diff = np.abs(mean_train - mean_train_reg).max()\n",
    "    std_diff = np.abs(std_train - std_train_reg).max()\n",
    "    \n",
    "    if mean_diff > 0.1 or std_diff > 0.1:\n",
    "        print(\"\\nUsing regression values for standardization parameters\")\n",
    "        mean_train = mean_train_reg\n",
    "        std_train = std_train_reg\n",
    "    else:\n",
    "        print(\"\\nDifferences are small. Keeping original calculated values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate Performance Metrics\n",
    "\n",
    "Now we'll calculate MAE (Mean Absolute Error) and MAPE (Mean Absolute Percentage Error) for each property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE and MAPE for each property\n",
    "metrics = []\n",
    "\n",
    "for col in target_cols:\n",
    "    pred_col = f\"{col}_pred_destd\"\n",
    "    \n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(merged_results[col], merged_results[pred_col])\n",
    "    \n",
    "    # Calculate MAPE (avoiding division by zero)\n",
    "    # We'll use the formula: MAPE = mean(|actual - predicted| / |actual|) * 100\n",
    "    non_zero_mask = merged_results[col] != 0\n",
    "    if non_zero_mask.sum() > 0:\n",
    "        mape = np.mean(np.abs((merged_results[col][non_zero_mask] - merged_results[pred_col][non_zero_mask]) / \n",
    "                              np.abs(merged_results[col][non_zero_mask]))) * 100\n",
    "    else:\n",
    "        mape = np.nan\n",
    "    \n",
    "    metrics.append({\n",
    "        'Property': col,\n",
    "        'MAE': mae,\n",
    "        'MAPE (%)': mape\n",
    "    })\n",
    "\n",
    "# Create a metrics dataframe\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize Predictions vs Ground Truth\n",
    "\n",
    "Let's create scatter plots to visualize the predicted values against the ground truth values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scatter plots for each property\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, col in enumerate(target_cols):\n",
    "    pred_col = f\"{col}_pred_destd\"\n",
    "    \n",
    "    axs[i].scatter(merged_results[col], merged_results[pred_col], alpha=0.5)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(merged_results[col].min(), merged_results[pred_col].min())\n",
    "    max_val = max(merged_results[col].max(), merged_results[pred_col].max())\n",
    "    axs[i].plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    axs[i].set_xlabel('Ground Truth')\n",
    "    axs[i].set_ylabel('Prediction')\n",
    "    axs[i].set_title(f'{col}\\nMAE: {metrics_df.loc[i, \"MAE\"]:.4f}, MAPE: {metrics_df.loc[i, \"MAPE (%)\"]:,.2f}%')\n",
    "    axs[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, 'output/D2R2_multi3/prediction_vs_ground_truth.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Error Distribution Analysis\n",
    "\n",
    "Let's visualize the distribution of prediction errors for each property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create error histograms for each property\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, col in enumerate(target_cols):\n",
    "    pred_col = f\"{col}_pred_destd\"\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = merged_results[col] - merged_results[pred_col]\n",
    "    \n",
    "    # Plot histogram\n",
    "    axs[i].hist(errors, bins=30, alpha=0.7)\n",
    "    axs[i].set_xlabel('Error (Ground Truth - Prediction)')\n",
    "    axs[i].set_ylabel('Frequency')\n",
    "    axs[i].set_title(f'{col} Error Distribution\\nMean: {errors.mean():.4f}, Std: {errors.std():.4f}')\n",
    "    axs[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(repo_root, 'output/D2R2_multi3/error_distribution.png'), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Processed Results\n",
    "\n",
    "Finally, let's save the processed results to a CSV file for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the merged results to a CSV file\n",
    "output_file = os.path.join(repo_root, 'output/D2R2_multi3/analyzed_results.csv')\n",
    "merged_results.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Save metrics to a CSV file\n",
    "metrics_file = os.path.join(repo_root, 'output/D2R2_multi3/performance_metrics.csv')\n",
    "metrics_df.to_csv(metrics_file, index=False)\n",
    "print(f\"Metrics saved to {metrics_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}